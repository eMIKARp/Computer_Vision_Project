{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMhsZ3vOnB4HaCa8b+ASElY"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BEjDieAdDOor",
        "outputId": "892e0046-0018-4e60-eb95-27132295c9bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU 0: Tesla T4 (UUID: GPU-3133706a-6150-1fb5-838d-c2ef6dd832f3)\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi -L"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing libraries needed for this project\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import seaborn as sns\n",
        "\n",
        "from tensorflow import keras\n",
        "\n",
        "from matplotlib import image as mpimg\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "\n",
        "import os\n",
        "import random\n",
        "import itertools\n",
        "import datetime\n",
        "import zipfile"
      ],
      "metadata": {
        "id": "N9ytbsouYVVg"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating functions that will be used in the project\n",
        "\n",
        "def load_and_prep_image(file_path,\n",
        "                        img_shape=224,\n",
        "                        scale=True):\n",
        "  \n",
        "  \"\"\"\n",
        "  Loads an image from a given file path and turns it into a tensor \n",
        "  of a predefined shape (img_shape,img_shape,3).\n",
        "\n",
        "  Args:\n",
        "  \n",
        "    file_path (str): string path to an image\n",
        "    img_shape (int): expected size of returned tensor (default = 224)\n",
        "    scale (bool): information whether to scale pixel values (default = True)\n",
        "  \n",
        "  Returns:\n",
        "\n",
        "    Tensor representation of a given image.\n",
        "  \"\"\"\n",
        "\n",
        "  img = tf.io.read_file(file_path)\n",
        "  img = tf.image.decode_jpeg(img)\n",
        "  img = tf.image.resize(img, [img_shape, img_shape])\n",
        "\n",
        "  if scale:\n",
        "    return img/255.\n",
        "  else:\n",
        "    return img\n",
        "\n",
        "\n",
        "def make_conf_matrix(y_true,\n",
        "                     y_pred,\n",
        "                     classes=None,\n",
        "                     figsize=(10, 10),\n",
        "                     text_size=15,\n",
        "                     norm=False,\n",
        "                     savefig=False): \n",
        "                        \n",
        "\n",
        "  \"\"\"\n",
        "  \n",
        "  Prepares a confusion matrix that compares predictions to ground truth labels.\n",
        "\n",
        "  Args:\n",
        "\n",
        "    y_true: array with ground truth labels\n",
        "    y_pred: array with predictions\n",
        "    classes: array of class names \n",
        "    figsize: expected size of figure (default = (10, 10)).\n",
        "    text_size: expected size of text on a figure (default=15).\n",
        "    norm: information whthere to normalize values or not (default=False).\n",
        "    savefig: information whether to save confusion matrix to a file (default=False).\n",
        "  \n",
        "  Returns:\n",
        "\n",
        "    Confusion matrix plot that compares predictions to ground truth labels.\n",
        "\n",
        "  \"\"\"  \n",
        "  \n",
        "  cm = confusion_matrix(y_true, y_pred)\n",
        "  cm_norm = cm.astype(\"float\") / cm.sum(axis=1)[:, np.newaxis]\n",
        "  n_classes = cm.shape[0]\n",
        "\n",
        "  fig, ax = plt.subplots(figsize=figsize)\n",
        "  cax = ax.matshow(cm, cmap=plt.cm.Blues)\n",
        "  fig.colorbar(cax)\n",
        "\n",
        "  if classes:\n",
        "    labels = classes\n",
        "  else:\n",
        "    labels = np.arange(cm.shape[0])\n",
        "  \n",
        "  ax.set(title=\"Confusion Matrix\",\n",
        "         xlabel=\"Predicted label\",\n",
        "         ylabel=\"True label\",\n",
        "         xticks=np.arange(n_classes),\n",
        "         yticks=np.arange(n_classes), \n",
        "         xticklabels=labels,\n",
        "         yticklabels=labels)\n",
        "  \n",
        "  ax.xaxis.set_label_position(\"bottom\")\n",
        "  ax.xaxis.tick_bottom()\n",
        "\n",
        "  threshold = (cm.max() + cm.min()) / 2.\n",
        "\n",
        "  for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "    if norm:\n",
        "      plt.text(j, i, f\"{cm[i, j]} ({cm_norm[i, j]*100:.1f}%)\",\n",
        "              horizontalalignment=\"center\",\n",
        "              color=\"white\" if cm[i, j] > threshold else \"black\",\n",
        "              size=text_size)\n",
        "    else:\n",
        "      plt.text(j, i, f\"{cm[i, j]}\",\n",
        "              horizontalalignment=\"center\",\n",
        "              color=\"white\" if cm[i, j] > threshold else \"black\",\n",
        "              size=text_size)\n",
        "\n",
        "  if savefig:\n",
        "    fig.savefig(\"confusion_matrix.png\")\n",
        "\n",
        "\n",
        "def pred_and_plot(model,\n",
        "                  file_path, \n",
        "                  class_names):\n",
        "  \"\"\"\n",
        "  Loads an image from a give path and makes a prediction on it with\n",
        "  provided model, plotting the image with its prediction at the end.\n",
        "\n",
        "  Args:\n",
        "    \n",
        "    model: pretrained model to perform predictions with\n",
        "    file_path: path to an image that we whant to make prediction on\n",
        "    class_names: names of available classes that we're predicting\n",
        "  \n",
        "  Returns:\n",
        "\n",
        "    Plots provided image along with it's predicted lable in the title\n",
        "  \"\"\"\n",
        "\n",
        "\n",
        "  img = load_and_prep_image(file_path)\n",
        "  pred = model.predict(tf.expand_dims(img, axis=0))\n",
        "\n",
        "  if len(pred[0]) > 1: \n",
        "    pred_class = class_names[pred.argmax()]\n",
        "  else:\n",
        "    pred_class = class_names[int(tf.round(pred)[0][0])]\n",
        "\n",
        "  plt.imshow(img)\n",
        "  plt.title(f\"Prediction: {pred_class}\")\n",
        "\n",
        "\n",
        "def create_tensorboard_callback(dir_name,\n",
        "                                experiment_name):\n",
        "  \n",
        "  \"\"\"\n",
        "  Creates a TensorBoard callback to store training log files.\n",
        "\n",
        "  Args:\n",
        "  \n",
        "    dir_name: target location to keep TensorBoard log files\n",
        "    experiment_name: name of experiment to distinguish log files between eachother\n",
        "\n",
        "  Returns:\n",
        "  \n",
        "    TensorBoard callback obejct to be used a one of parameters during\n",
        "    model training process.\n",
        "  \"\"\"\n",
        "\n",
        "  log_dir = dir_name + \"/\" + experiment_name + \"/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "  tb_callback = tf.keras.callbacks.TensorBoard(\n",
        "      log_dir=log_dir\n",
        "  )\n",
        "  \n",
        "  return tb_callback\n",
        "\n",
        "\n",
        "\n",
        "def plot_training_curves(history):\n",
        "\n",
        "  \"\"\"\n",
        "  Plots separate training curves for training and validation set.\n",
        "\n",
        "  Args:\n",
        "  \n",
        "    history: TensorFlow model History object\n",
        "  \n",
        "  Returns:\n",
        "  \n",
        "    Plot of separate training curves for training and validation set.\n",
        "  \"\"\" \n",
        "  \n",
        "  loss = history.history['loss']\n",
        "  val_loss = history.history['val_loss']\n",
        "\n",
        "  accuracy = history.history['accuracy']\n",
        "  val_accuracy = history.history['val_accuracy']\n",
        "\n",
        "  epochs = range(len(history.history['loss']))\n",
        "\n",
        "  plt.figure(figsize = (24,7))\n",
        "\n",
        "  plt.subplot(1,2,1)\n",
        "  plt.plot(epochs, loss, label='training_loss')\n",
        "  plt.plot(epochs, val_loss, label='val_loss')\n",
        "  plt.title('Loss')\n",
        "  plt.xlabel('Epochs')\n",
        "  plt.legend()\n",
        "\n",
        "  plt.subplot(1,2,2)\n",
        "  plt.plot(epochs, accuracy, label='training_accuracy')\n",
        "  plt.plot(epochs, val_accuracy, label='val_accuracy')\n",
        "  plt.title('Accuracy')\n",
        "  plt.xlabel('Epochs')\n",
        "  plt.legend();\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def compare_histories(org_history,\n",
        "                     new_history, \n",
        "                     init_epochs=5):\n",
        "  \n",
        "    \"\"\"\n",
        "    Compares two TensorFlow model History objects.\n",
        "    \n",
        "    Args:\n",
        "\n",
        "      org_history: History object from original model\n",
        "      new_history: History object from fine tuned model\n",
        "      init_epochs: Number of epochs in initial training stage\n",
        "\n",
        "\n",
        "    Returns:\n",
        "\n",
        "    Plots showing training curves of training and valuation set,\n",
        "    before and after tunning phase\n",
        "\n",
        "    \"\"\"\n",
        "    \n",
        "    acc = org_history.history[\"accuracy\"]\n",
        "    loss = org_history.history[\"loss\"]\n",
        "\n",
        "    val_acc = org_history.history[\"val_accuracy\"]\n",
        "    val_loss = org_history.history[\"val_loss\"]\n",
        "\n",
        "    total_acc = acc + new_history.history[\"accuracy\"]\n",
        "    total_loss = loss + new_history.history[\"loss\"]\n",
        "\n",
        "    total_val_acc = val_acc + new_history.history[\"val_accuracy\"]\n",
        "    total_val_loss = val_loss + new_history.history[\"val_loss\"]\n",
        "\n",
        "    plt.figure(figsize=(8, 8))\n",
        "    plt.subplot(2, 1, 1)\n",
        "    plt.plot(total_acc, label='Training Accuracy')\n",
        "    plt.plot(total_val_acc, label='Validation Accuracy')\n",
        "    plt.plot([init_epochs-1, \n",
        "              init_epochs-1],\n",
        "              plt.ylim(),\n",
        "             label='Start Fine Tuning')\n",
        "    \n",
        "    plt.legend(loc='lower right')\n",
        "    plt.title('Training and Validation Accuracy')\n",
        "\n",
        "    plt.subplot(2, 1, 2)\n",
        "    plt.plot(total_loss, label='Training Loss')\n",
        "    plt.plot(total_val_loss, label='Validation Loss')\n",
        "    plt.plot([init_epochs-1,\n",
        "              init_epochs-1],\n",
        "              plt.ylim(),\n",
        "             label='Start Fine Tuning')\n",
        "    \n",
        "    plt.legend(loc='upper right')\n",
        "    plt.title('Training and Validation Loss')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def unzip_data(file_path):\n",
        "  \n",
        "  \"\"\"\n",
        "  Extracts file from a file_path into current working directory.\n",
        "\n",
        "  Args:\n",
        "    \n",
        "    file_path (str): a file path to a file that needs to be extracted.\n",
        "  \n",
        "  Returns:\n",
        "\n",
        "    Extracts file from a file_path into current working directory.\n",
        "\n",
        "  \"\"\"\n",
        "  \n",
        "  zip_ref = zipfile.ZipFile(file_path, \"r\")\n",
        "  zip_ref.extractall()\n",
        "  zip_ref.close()\n",
        "\n",
        "\n",
        "\n",
        "def walk_through_dir(dir_path):\n",
        "\n",
        "  \"\"\"\n",
        "  Walks through dir_path returning its contents.\n",
        "\n",
        "  Args:\n",
        "  \n",
        "    dir_path (str): target directory\n",
        "  \n",
        "  Returns:\n",
        "\n",
        "    Prints out the number and names of subdirectories, along with\n",
        "    number of files in each subdirectory\n",
        "  \"\"\"\n",
        "  \n",
        "  for dir_path, dir_names, file_names in os.walk(dir_path):\n",
        "    print(f\"There are {len(dir_names)} directories and {len(file_names)} images in '{dir_path}'.\")\n",
        "\n",
        "\n",
        "\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "\n",
        "def calculate_results(y_true, y_pred):\n",
        "  \"\"\"\n",
        "  Calculates model accuracy, precision, recall and f1 score \n",
        "  of a binary classification model.\n",
        "\n",
        "  Args:\n",
        "      y_true: true labels in the form of a 1D array\n",
        "      y_pred: predicted labels in the form of a 1D array\n",
        "\n",
        "  Returns a dictionary of accuracy, precision, recall, f1-score.\n",
        "  \"\"\"\n",
        "  # Calculate model accuracy\n",
        "  model_accuracy = accuracy_score(y_true, y_pred) * 100\n",
        "  # Calculate model precision, recall and f1 score using \"weighted average\n",
        "  model_precision, model_recall, model_f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"weighted\")\n",
        "  model_results = {\"accuracy\": model_accuracy,\n",
        "                  \"precision\": model_precision,\n",
        "                  \"recall\": model_recall,\n",
        "                  \"f1\": model_f1}\n",
        "                  \n",
        "  return model_results\n"
      ],
      "metadata": {
        "id": "WdTycHe2WlCv"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HGBw584UyVrY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}